{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PAC_Dataset import PACDataset, ConcatDataset, ShuffleClassDataset, ConcatDomainDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import logging.handlers\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE] [--learning_rate LEARNING_RATE] [--momentum MOMENTUM] [--gpu_num GPU_NUM]\n",
      "                             [--seed SEED] [--save_path SAVE_PATH] [--subfolder SUBFOLDER] [--wtarget WTARGET]\n",
      "                             [--model_save_period MODEL_SAVE_PERIOD] [--epochs EPOCHS] [--dann_weight DANN_WEIGHT]\n",
      "                             [--start_shuffle_dann START_SHUFFLE_DANN] [--is_shuffle IS_SHUFFLE] [--domains DOMAINS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/tianqinl/.local/share/jupyter/runtime/kernel-8103d7dc-8490-4e37-b949-c3e5d53fa87b.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianqinl/anaconda3/envs/pgm/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Domain adaptation')\n",
    "parser.add_argument(\"--batch_size\", type=int, default=\"400\", help=\"batch size\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=1e-3, help=\"learning rate\")\n",
    "parser.add_argument(\"--momentum\", type=float, default=0.5, help=\"momentum\")\n",
    "parser.add_argument(\"--gpu_num\", type=int, default=0, help=\"gpu num\")\n",
    "parser.add_argument(\"--seed\", type=int, default=123, help=\"munually set seed\")\n",
    "parser.add_argument(\"--save_path\", type=str, default=\"../train_related\", help=\"save path\")\n",
    "parser.add_argument(\"--subfolder\", type=str, default='test', help=\"subfolder name\")\n",
    "parser.add_argument(\"--wtarget\", type=float, default=0.7, help=\"target weight\")\n",
    "parser.add_argument(\"--model_save_period\", type=int, default=2, help=\"save period\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=2000, help=\"label shuffling\")\n",
    "parser.add_argument(\"--dann_weight\", type=float, default=1, help=\"weight for label shuffling\")\n",
    "parser.add_argument(\"--start_shuffle_dann\", type=int, default=100, help=\"when to start shuffling\")\n",
    "parser.add_argument(\"--is_shuffle\", type=int, default=1, help=\"no shuffle if 0\")\n",
    "parser.add_argument(\"--domains\", type=int, default=2, help=\"how many source domain\")\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "# snap shot of py file and command\n",
    "python_file_name = sys.argv[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# local only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # local only\n",
    "# class local_args:\n",
    "#     def __init__(self, **entries):\n",
    "#         self.__dict__.update(entries)\n",
    "        \n",
    "# args = local_args(**{\n",
    "#     'batch_size': 100,\n",
    "#     'learning_rate': 1e-3,\n",
    "#     'momentum': 0.5,\n",
    "#     'gpu_num': 0,\n",
    "#     'seed': 123,\n",
    "#     'save_path': \"../train_related\",\n",
    "#     'epochs': 20,\n",
    "#     'subfolder': \"test\",\n",
    "#     'wtarget': 0.7,\n",
    "#     'dann_weight': 1,\n",
    "#     'model_save_period': 2,\n",
    "#     'start_shuffle_dann': 0,\n",
    "#     'is_shuffle': 1,\n",
    "#     'domains': 3,\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda:{}'.format(args.gpu_num) if torch.cuda.is_available() else 'cpu')\n",
    "# seed\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "cudnn.deterministic = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda:{}'.format(args.gpu_num) if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "\n",
    "model_sub_folder = args.subfolder + '/shuffle_weight_%f_learningrate_%f_startsepoch_%i_isshuffle_%i_domains_%i'%(args.dann_weight, args.learning_rate, args.start_shuffle_dann, args.is_shuffle, args.domains)\n",
    "save_folder = os.path.join(args.save_path, model_sub_folder)\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed source testing bug\n",
      "batch_size: 100\n",
      "learning_rate: 0.001\n",
      "momentum: 0.5\n",
      "gpu_num: 0\n",
      "seed: 123\n",
      "save_path: ../train_related\n",
      "epochs: 20\n",
      "subfolder: test\n",
      "wtarget: 0.7\n",
      "dann_weight: 1\n",
      "model_save_period: 2\n",
      "start_shuffle_dann: 0\n",
      "is_shuffle: 0\n",
      "domains: 2\n",
      "Training Save Path: ../train_related/test/shuffle_weight_1.000000_learningrate_0.001000_startsepoch_0_isshuffle_0_domains_2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'python_file_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-29a125858402>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Save Path: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'executed.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mcommands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mcommands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'python_file_name' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logfile_path = os.path.join(save_folder, 'logfile.log')\n",
    "if os.path.isfile(logfile_path):\n",
    "    os.remove(logfile_path)\n",
    "    \n",
    "file_log_handler = logging.FileHandler(logfile_path)\n",
    "logger.addHandler(file_log_handler)\n",
    "\n",
    "stdout_log_handler = logging.StreamHandler(sys.stdout)\n",
    "logger.addHandler(stdout_log_handler)\n",
    "logger.info(\"Fixed source testing bug\")\n",
    "attrs = vars(args)\n",
    "for item in attrs.items():\n",
    "    logger.info(\"%s: %s\"%item)\n",
    "logger.info(\"Training Save Path: {}\".format(save_folder))\n",
    "\n",
    "copyfile(python_file_name, os.path.join(save_folder, 'executed.py'))\n",
    "commands = ['python']\n",
    "commands.extend(sys.argv)\n",
    "with open(os.path.join(save_folder, 'command.log'), 'w') as f:\n",
    "    f.write(' '.join(commands))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading photo train data from /home/tianqinl/Code/PGM-project/data/pacs/cache\n",
      "Loading photo test data from /home/tianqinl/Code/PGM-project/data/pacs/cache\n",
      "Loading art_painting train data from /home/tianqinl/Code/PGM-project/data/pacs/cache\n",
      "Loading art_painting test data from /home/tianqinl/Code/PGM-project/data/pacs/cache\n",
      "Loading cartoon train data from /home/tianqinl/Code/PGM-project/data/pacs/cache\n",
      "Loading cartoon test data from /home/tianqinl/Code/PGM-project/data/pacs/cache\n",
      "Loading sketch train data from /home/tianqinl/Code/PGM-project/data/pacs/cache\n",
      "Loading sketch test data from /home/tianqinl/Code/PGM-project/data/pacs/cache\n"
     ]
    }
   ],
   "source": [
    "p_dataset_train = PACDataset('p', split = 'train')\n",
    "p_dataset_test = PACDataset('p', split = 'test')\n",
    "a_dataset_train = PACDataset('a', split = 'train')\n",
    "a_dataset_test = PACDataset('a', split = 'test')\n",
    "c_dataset_train = PACDataset('c', split = 'train')\n",
    "c_dataset_test = PACDataset('c', split = 'test')\n",
    "s_dataset_train = PACDataset('s', split = 'train')\n",
    "s_dataset_test = PACDataset('s', split = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual comfirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_dataloader = DataLoader(p_dataset_test, batch_size=args.batch_size, shuffle=True)\n",
    "# # pacs\n",
    "# examples = enumerate(p_dataloader)\n",
    "# batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "\n",
    "# fig = plt.figure()\n",
    "# for i in range(6):\n",
    "#     plt.subplot(2,3,i+1)\n",
    "#     plt.tight_layout()\n",
    "#     plt.imshow(example_data[i].permute(1, 2, 0))\n",
    "#     plt.title(\"Ground Truth: {}-{}\".format(example_targets[i], p_dataset_test.category_dict[int(example_targets[i])]))\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train_dataloader = DataLoader(p_dataset_train, batch_size=args.batch_size, shuffle=True, pin_memory=True)\n",
    "p_test_dataloader = DataLoader(p_dataset_test, batch_size=args.batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "a_train_dataloader = DataLoader(a_dataset_train, batch_size=args.batch_size, shuffle=True, pin_memory=True)\n",
    "a_test_dataloader = DataLoader(a_dataset_test, batch_size=args.batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "c_train_dataloader = DataLoader(c_dataset_train, batch_size=args.batch_size, shuffle=True, pin_memory=True)\n",
    "c_test_dataloader = DataLoader(c_dataset_test, batch_size=args.batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "s_train_dataloader = DataLoader(s_dataset_train, batch_size=args.batch_size, shuffle=True, pin_memory=True)\n",
    "s_test_dataloader = DataLoader(s_dataset_test, batch_size=args.batch_size, shuffle=True, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling Domain label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_train_tuple = (p_dataset_train, a_dataset_train, s_dataset_train)\n",
    "# source_test_tuple = (p_dataset_test, a_dataset_test, s_dataset_test)\n",
    "\n",
    "shuffle_domain_train_dataset = ShuffleClassDataset(ConcatDomainDataset(p_dataset_train, a_dataset_train, s_dataset_train, c_dataset_train))\n",
    "# shuffle_domain_test = ShuffleClassDataset(ConcatDomainDataset(source_test_tuple))\n",
    "\n",
    "shuffle_domain_train_dataloader = DataLoader(shuffle_domain_train_dataset, batch_size=args.batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=10, padding=2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=11, padding=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(55*55*20, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 300)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 6, stride=2, padding=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 6, stride=2, padding=2))\n",
    "        x = x.view(-1, 55*55*20)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, d_in, d_h1, d_h2, d_out, dp=0.2):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_in, d_h1)\n",
    "        self.ln1 = nn.LayerNorm(d_h1)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.dropout1 = nn.Dropout(dp)\n",
    "        self.fc2 = nn.Linear(d_h1, d_h2)\n",
    "        self.ln2 = nn.LayerNorm(d_h2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.dropout2 = nn.Dropout(dp)\n",
    "        self.fc3 = nn.Linear(d_h2, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.ln1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.ln2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def before_lastlinear(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.ln1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.ln2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "    elif type(m) == nn.LayerNorm:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FNN(\n",
       "  (fc1): Linear(in_features=300, out_features=1000, bias=True)\n",
       "  (ln1): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
       "  (relu1): ReLU(inplace=True)\n",
       "  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "  (fc2): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (ln2): LayerNorm((500,), eps=1e-05, elementwise_affine=True)\n",
       "  (relu2): ReLU(inplace=True)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (fc3): Linear(in_features=500, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda:{}'.format(args.gpu_num) if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "encoder = Encoder().to(device)\n",
    "CNet = FNN(d_in=300, d_h1=1000, d_h2=500, d_out=10, dp=0.2).to(device)\n",
    "DomainCNet = FNN(d_in=300, d_h1=1000, d_h2=500, d_out=2, dp=0.2).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizerEncoder = optim.Adam(encoder.parameters(), lr=args.learning_rate)\n",
    "optimizerCNet = optim.Adam(CNet.parameters(), lr=args.learning_rate)\n",
    "optimizerDomainCNet = optim.Adam(DomainCNet.parameters(), lr=args.learning_rate)\n",
    "\n",
    "criterion_classifier = nn.CrossEntropyLoss().to(device)\n",
    "# criterion_adverisal = \n",
    "\n",
    "encoder.apply(weights_init)\n",
    "CNet.apply(weights_init)\n",
    "DomainCNet.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  5.28it/s]\n",
      "100%|██████████| 15/15 [00:02<00:00,  5.27it/s]\n",
      "100%|██████████| 28/28 [00:05<00:00,  5.26it/s]\n",
      "100%|██████████| 70/70 [00:13<00:00,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start update Encoder using shuffling loss!\n",
      "Epoch 0: source 1 loss: 19.38962960243225; source 2 loss: 28.51963710784912; source 3 loss: 51.51281821727753; shuffle loss: 135.11442917585373; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 6/6 [00:00<00:00, 12.18it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 15.57it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 14.14it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 15.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; train source 1 train acc: 0.3721129170230967source 2 train acc: 0.2421493370551291source 3 train acc: 0.18327272727272728shuffle domain acc: 0.18935926773455378\n",
      "Epoch: 0; test source 1 test acc: 0.1277445109780439; source 2 test acc: 0.0943089430894309; source 3 test acc: 0.21628498727735368; target test acc: 0.14630681818181818; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  5.34it/s]\n",
      "100%|██████████| 15/15 [00:02<00:00,  5.24it/s]\n",
      "100%|██████████| 28/28 [00:05<00:00,  5.26it/s]\n",
      "100%|██████████| 70/70 [00:13<00:00,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: source 1 loss: 24.24432396888733; source 2 loss: 28.86469876766205; source 3 loss: 51.74883544445038; shuffle loss: 105.26017189025879; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 6/6 [00:00<00:00, 12.44it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 15.49it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 13.87it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 14.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; train source 1 train acc: 0.1941830624465355source 2 train acc: 0.2030704815073273source 3 train acc: 0.1890909090909091shuffle domain acc: 0.1853546910755149\n",
      "Epoch: 1; test source 1 test acc: 0.11976047904191617; source 2 test acc: 0.14959349593495935; source 3 test acc: 0.16793893129770993; target test acc: 0.20454545454545456; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:02<00:00,  5.32it/s]\n",
      "100%|██████████| 15/15 [00:02<00:00,  5.19it/s]\n",
      "100%|██████████| 28/28 [00:05<00:00,  5.25it/s]\n",
      "100%|██████████| 70/70 [00:13<00:00,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: source 1 loss: 26.41774606704712; source 2 loss: 29.42128026485443; source 3 loss: 51.02722978591919; shuffle loss: 192.53104728460312; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 6/6 [00:00<00:00, 12.46it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 15.49it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 14.27it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 14.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2; train source 1 train acc: 0.15654405474764757source 2 train acc: 0.20237264480111655source 3 train acc: 0.19527272727272726shuffle domain acc: 0.17391304347826086\n",
      "Epoch: 2; test source 1 test acc: 0.1277445109780439; source 2 test acc: 0.0943089430894309; source 3 test acc: 0.21628498727735368; target test acc: 0.14630681818181818; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  5.34it/s]\n",
      "100%|██████████| 15/15 [00:02<00:00,  5.23it/s]\n",
      "100%|██████████| 28/28 [00:05<00:00,  5.25it/s]\n",
      "100%|██████████| 70/70 [00:13<00:00,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: source 1 loss: 25.84270215034485; source 2 loss: 29.07854664325714; source 3 loss: 52.39515948295593; shuffle loss: 256.6626519560814; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 6/6 [00:00<00:00, 12.51it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 14.93it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 14.23it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 15.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3; train source 1 train acc: 0.1317365269461078source 2 train acc: 0.22191207257501744source 3 train acc: 0.15527272727272728shuffle domain acc: 0.20437643020594964\n",
      "Epoch: 3; test source 1 test acc: 0.10379241516966067; source 2 test acc: 0.12357723577235773; source 3 test acc: 0.18575063613231552; target test acc: 0.14488636363636365; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:02<00:00,  5.27it/s]\n",
      "100%|██████████| 15/15 [00:02<00:00,  5.07it/s]\n",
      "100%|██████████| 28/28 [00:05<00:00,  5.24it/s]\n",
      "100%|██████████| 70/70 [00:13<00:00,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: source 1 loss: 25.283286452293396; source 2 loss: 29.58432626724243; source 3 loss: 51.804585576057434; shuffle loss: 65.33479577302933; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 6/6 [00:00<00:00, 12.24it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 14.89it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 13.28it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 14.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4; train source 1 train acc: 0.18733960650128315source 2 train acc: 0.19050942079553385source 3 train acc: 0.16727272727272727shuffle domain acc: 0.16762013729977115\n",
      "Epoch: 4; test source 1 test acc: 0.11377245508982035; source 2 test acc: 0.1983739837398374; source 3 test acc: 0.20441051738761662; target test acc: 0.16051136363636365; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  5.24it/s]\n",
      "100%|██████████| 15/15 [00:02<00:00,  5.20it/s]\n",
      "100%|██████████| 28/28 [00:05<00:00,  5.26it/s]\n",
      "100%|██████████| 70/70 [00:13<00:00,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: source 1 loss: 25.655292868614197; source 2 loss: 29.149227380752563; source 3 loss: 52.54665529727936; shuffle loss: 44525.61228364706; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 6/6 [00:00<00:00, 12.47it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 15.03it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 14.37it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 15.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5; train source 1 train acc: 0.12660393498716851source 2 train acc: 0.2226099092812282source 3 train acc: 0.18036363636363636shuffle domain acc: 0.1709096109839817\n",
      "Epoch: 5; test source 1 test acc: 0.11976047904191617; source 2 test acc: 0.14959349593495935; source 3 test acc: 0.16793893129770993; target test acc: 0.20454545454545456; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:02<00:00,  5.37it/s]\n",
      "100%|██████████| 15/15 [00:02<00:00,  5.23it/s]\n",
      "100%|██████████| 28/28 [00:05<00:00,  5.21it/s]\n",
      "100%|██████████| 70/70 [00:13<00:00,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: source 1 loss: 27.71783697605133; source 2 loss: 28.918330907821655; source 3 loss: 53.945117592811584; shuffle loss: 156.61048060655594; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 6/6 [00:00<00:00, 12.38it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 15.45it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 14.06it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 15.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6; train source 1 train acc: 0.11206159110350727source 2 train acc: 0.21772505233775297source 3 train acc: 0.13127272727272726shuffle domain acc: 0.16819221967963388\n",
      "Epoch: 6; test source 1 test acc: 0.11377245508982035; source 2 test acc: 0.1983739837398374; source 3 test acc: 0.20441051738761662; target test acc: 0.16051136363636365; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  5.38it/s]\n",
      "100%|██████████| 15/15 [00:02<00:00,  5.21it/s]\n",
      "100%|██████████| 28/28 [00:05<00:00,  5.24it/s]\n",
      "100%|██████████| 70/70 [00:13<00:00,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: source 1 loss: 26.143966913223267; source 2 loss: 29.14591872692108; source 3 loss: 53.5934693813324; shuffle loss: 422.4862967133522; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 6/6 [00:00<00:00, 11.75it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 15.30it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 13.43it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 15.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7; train source 1 train acc: 0.11377245508982035source 2 train acc: 0.1884159106769016source 3 train acc: 0.14836363636363636shuffle domain acc: 0.16719107551487414\n",
      "Epoch: 7; test source 1 test acc: 0.11976047904191617; source 2 test acc: 0.14959349593495935; source 3 test acc: 0.16793893129770993; target test acc: 0.20454545454545456; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12/12 [00:02<00:00,  5.41it/s]\n",
      "100%|██████████| 15/15 [00:02<00:00,  5.22it/s]\n",
      "100%|██████████| 28/28 [00:05<00:00,  5.25it/s]\n",
      "100%|██████████| 70/70 [00:13<00:00,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: source 1 loss: 25.50056552886963; source 2 loss: 29.374919891357422; source 3 loss: 53.333062171936035; shuffle loss: 47.743288576602936; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 6/6 [00:00<00:00, 12.53it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 15.64it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 14.13it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 15.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8; train source 1 train acc: 0.1086398631308811source 2 train acc: 0.21004884856943476source 3 train acc: 0.1509090909090909shuffle domain acc: 0.16719107551487414\n",
      "Epoch: 8; test source 1 test acc: 0.11976047904191617; source 2 test acc: 0.14959349593495935; source 3 test acc: 0.16793893129770993; target test acc: 0.20454545454545456; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  5.42it/s]\n",
      " 47%|████▋     | 7/15 [00:01<00:01,  4.59it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-39c87ecf5dcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0msource_x_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_x_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0msource_acc_2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msource_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mloss_source_2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# acc store\n",
    "source_acc_1_ = []\n",
    "source_acc_2_ = []\n",
    "source_acc_3_ = []\n",
    "source_test_acc_1_ = []\n",
    "source_test_acc_2_ = []\n",
    "source_test_acc_3_ = []\n",
    "target_test_acc_ = []\n",
    "domain_acc_ = []\n",
    "\n",
    "#loss store\n",
    "accumulate_domain_loss_ = []\n",
    "loss_source_1_ = []\n",
    "loss_source_2_ = []\n",
    "loss_source_3_ = []\n",
    "\n",
    "logger.info('Started Training')\n",
    "\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    log_loss_str = \"\"\n",
    "    log_train_acc_str = \"\"\n",
    "    log_test_acc_str = \"\"\n",
    "    # update classifier\n",
    "    # on source 1 domain p\n",
    "    CNet.train()\n",
    "    encoder.train()\n",
    "    source_acc_1 = 0.0\n",
    "    num_datas = 0.0\n",
    "    loss_source_1 = 0.0\n",
    "    for batch_id, (source_x, source_y) in tqdm(enumerate(p_train_dataloader), total=len(p_train_dataloader)):\n",
    "        optimizerCNet.zero_grad()\n",
    "        optimizerEncoder.zero_grad()\n",
    "        source_x = source_x.to(device).float()\n",
    "        source_y = source_y.to(device)\n",
    "        num_datas += source_x.size(0)\n",
    "        source_x_embedding = encoder(source_x)\n",
    "        pred = CNet(source_x_embedding)\n",
    "        source_acc_1 += (pred.argmax(-1) == source_y).sum().item()\n",
    "        loss = criterion_classifier(pred, source_y)\n",
    "        loss_source_1 += loss.item()\n",
    "        loss.backward()\n",
    "        optimizerCNet.step()\n",
    "        optimizerEncoder.step()\n",
    "        \n",
    "        \n",
    "    source_acc_1 = source_acc_1 / num_datas\n",
    "    source_acc_1_.append(source_acc_1)\n",
    "    loss_source_1_.append(loss_source_1)\n",
    "    \n",
    "    log_loss_str += \"source 1 loss: {}; \".format(loss_source_1)\n",
    "    log_train_acc_str += \"source 1 train acc: {}; \".format(source_acc_1)\n",
    "    \n",
    "    if args.domains >= 2:\n",
    "        # on source 2 domain c\n",
    "        CNet.train()\n",
    "        encoder.train()\n",
    "        source_acc_2 = 0.0\n",
    "        num_datas = 0.0\n",
    "        loss_source_2 = 0.0\n",
    "        for batch_id, (source_x, source_y) in tqdm(enumerate(a_train_dataloader), total=len(a_train_dataloader)):\n",
    "            optimizerCNet.zero_grad()\n",
    "            optimizerEncoder.zero_grad()\n",
    "            source_x = source_x.to(device).float()\n",
    "            source_y = source_y.to(device)\n",
    "            num_datas += source_x.size(0)\n",
    "            source_x_embedding = encoder(source_x)\n",
    "            pred = CNet(source_x_embedding)\n",
    "            source_acc_2 += (pred.argmax(-1) == source_y).sum().item()\n",
    "            loss = criterion_classifier(pred, source_y)\n",
    "            loss_source_2 += loss.item()\n",
    "            loss.backward()\n",
    "            optimizerCNet.step()\n",
    "            optimizerEncoder.step()\n",
    "\n",
    "\n",
    "        source_acc_2 = source_acc_2 / num_datas\n",
    "        source_acc_2_.append(source_acc_2)\n",
    "        loss_source_2_.append(loss_source_2)\n",
    "        log_loss_str += \"source 2 loss: {}; \".format(loss_source_2)\n",
    "        log_train_acc_str += \"source 2 train acc: {}; \".format(source_acc_2)\n",
    "        \n",
    "    if args.domains >= 3:\n",
    "        # on source 3 domain s\n",
    "        CNet.train()\n",
    "        encoder.train()\n",
    "        source_acc_3 = 0.0\n",
    "        num_datas = 0.0\n",
    "        loss_source_3 = 0.0\n",
    "        for batch_id, (source_x, source_y) in tqdm(enumerate(s_train_dataloader), total=len(s_train_dataloader)):\n",
    "            optimizerCNet.zero_grad()\n",
    "            optimizerEncoder.zero_grad()\n",
    "            source_x = source_x.to(device).float()\n",
    "            source_y = source_y.to(device)\n",
    "            num_datas += source_x.size(0)\n",
    "            source_x_embedding = encoder(source_x)\n",
    "            pred = CNet(source_x_embedding)\n",
    "            source_acc_3 += (pred.argmax(-1) == source_y).sum().item()\n",
    "            loss = criterion_classifier(pred, source_y)\n",
    "            loss_source_3 += loss.item()\n",
    "            loss.backward()\n",
    "            optimizerCNet.step()\n",
    "            optimizerEncoder.step()\n",
    "\n",
    "\n",
    "        source_acc_3 = source_acc_3 / num_datas\n",
    "        source_acc_3_.append(source_acc_3)\n",
    "        loss_source_3_.append(loss_source_3)\n",
    "        log_loss_str += \"source 3 loss: {}; \".format(loss_source_3)\n",
    "        log_train_acc_str += \"source 3 train acc: {}; \".format(source_acc_3)\n",
    "        \n",
    "    # domain shuffle\n",
    "    if args.is_shuffle != 0:\n",
    "        accumulate_loss = 0.0\n",
    "        domain_acc = 0.0\n",
    "        DomainCNet.train()\n",
    "        encoder.train()\n",
    "        num_datas = 0.0\n",
    "        for batch_id, (adv_x, adv_y) in tqdm(enumerate(shuffle_domain_train_dataloader), total=len(shuffle_domain_train_dataloader)):\n",
    "            optimizerCNet.zero_grad()\n",
    "            optimizerEncoder.zero_grad()\n",
    "            adv_x = adv_x.to(device).float()\n",
    "            adv_y = adv_y.to(device)\n",
    "            num_datas += adv_x.size(0)\n",
    "            adv_x_embedding = encoder(adv_x)\n",
    "            pred = DomainCNet(adv_x_embedding)\n",
    "            domain_acc += (pred.argmax(-1) == adv_y).sum().item()\n",
    "            # adv_acc += (pred.argmax(-1) == adv_y).sum().item()\n",
    "            loss = args.dann_weight * criterion_classifier(pred, adv_y)\n",
    "            accumulate_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizerDomainCNet.step()\n",
    "            if epoch >= args.start_shuffle_dann:\n",
    "                optimizerEncoder.step()    \n",
    "        domain_acc = domain_acc / num_datas\n",
    "        domain_acc_.append(domain_acc)\n",
    "        log_train_acc_str += \"shuffle domain acc: {}\".format(domain_acc)\n",
    "        accumulate_domain_loss_.append(accumulate_loss) \n",
    "        if epoch == args.start_shuffle_dann:\n",
    "            logger.info(\"Start update Encoder using shuffling loss!\")\n",
    "        \n",
    "        log_loss_str += \"shuffle loss: {}; \".format(accumulate_loss)\n",
    "    \n",
    "    logger.info(\"Epoch {}: \".format(epoch) + log_loss_str)\n",
    "    \n",
    "          \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # eval on source   \n",
    "\n",
    "    CNet.eval()\n",
    "    encoder.eval()\n",
    "    \n",
    "    \n",
    "    source_test_acc_1 = 0.0\n",
    "    num_datas = 0.0    \n",
    "    for batch_id, (source_x, source_y) in tqdm(enumerate(p_test_dataloader), total=len(p_test_dataloader)):\n",
    "        optimizerCNet.zero_grad()\n",
    "        optimizerEncoder.zero_grad()\n",
    "        source_x = source_x.to(device).float()\n",
    "        source_y = source_y.to(device)\n",
    "        num_datas += source_x.size(0)\n",
    "        source_x_embedding = encoder(source_x)\n",
    "        pred = CNet(source_x_embedding)\n",
    "        source_test_acc_1 += (pred.argmax(-1) == source_y).sum().item()\n",
    "        \n",
    "    source_test_acc_1 = source_test_acc_1 / num_datas\n",
    "    source_test_acc_1_.append(source_test_acc_1)\n",
    "    log_test_acc_str += \"source 1 test acc: {}; \".format(source_test_acc_1)\n",
    "    \n",
    "    if args.domains >= 2:\n",
    "        source_test_acc_2 = 0.0\n",
    "        num_datas = 0.0    \n",
    "        for batch_id, (source_x, source_y) in tqdm(enumerate(a_test_dataloader), total=len(a_test_dataloader)):\n",
    "            optimizerCNet.zero_grad()\n",
    "            optimizerEncoder.zero_grad()\n",
    "            source_x = source_x.to(device).float()\n",
    "            source_y = source_y.to(device)\n",
    "            num_datas += source_x.size(0)\n",
    "            source_x_embedding = encoder(source_x)\n",
    "            pred = CNet(source_x_embedding)\n",
    "            source_test_acc_2 += (pred.argmax(-1) == source_y).sum().item()\n",
    "\n",
    "        source_test_acc_2 = source_test_acc_2 / num_datas\n",
    "        source_test_acc_2_.append(source_test_acc_2)\n",
    "        log_test_acc_str += \"source 2 test acc: {}; \".format(source_test_acc_2)\n",
    "    \n",
    "    if args.domains >= 3:\n",
    "        source_test_acc_3 = 0.0\n",
    "        num_datas = 0.0    \n",
    "        for batch_id, (source_x, source_y) in tqdm(enumerate(s_test_dataloader), total=len(s_test_dataloader)):\n",
    "            optimizerCNet.zero_grad()\n",
    "            optimizerEncoder.zero_grad()\n",
    "            source_x = source_x.to(device).float()\n",
    "            source_y = source_y.to(device)\n",
    "            num_datas += source_x.size(0)\n",
    "            source_x_embedding = encoder(source_x)\n",
    "            pred = CNet(source_x_embedding)\n",
    "            source_test_acc_3 += (pred.argmax(-1) == source_y).sum().item()\n",
    "\n",
    "        source_test_acc_3 = source_test_acc_3 / num_datas\n",
    "        source_test_acc_3_.append(source_test_acc_3)\n",
    "        log_test_acc_str += \"source 3 test acc: {}; \".format(source_test_acc_3)\n",
    "    # eval on target \n",
    "    num_datas = 0.0\n",
    "    target_test_acc = 0.0\n",
    "    for batch_id, (target_x, target_y) in tqdm(enumerate(c_test_dataloader), total=len(c_test_dataloader)):\n",
    "        optimizerCNet.zero_grad()\n",
    "        optimizerEncoder.zero_grad()\n",
    "        target_x = target_x.to(device).float()\n",
    "        target_y = target_y.to(device)\n",
    "        num_datas += target_x.size(0)\n",
    "        target_x_embedding = encoder(target_x)\n",
    "        pred = CNet(target_x_embedding)\n",
    "        target_test_acc += (pred.argmax(-1) == target_y).sum().item()\n",
    "    \n",
    "    target_test_acc = target_test_acc / num_datas\n",
    "    target_test_acc_.append(target_test_acc)\n",
    "    log_test_acc_str += \"target test acc: {}; \".format(target_test_acc)\n",
    "    \n",
    "    if epoch % args.model_save_period == 0:\n",
    "        torch.save(DomainCNet.state_dict(), os.path.join(save_folder, 'DomainCNet_%i.t7'%(epoch+1)))\n",
    "        torch.save(encoder.state_dict(), os.path.join(save_folder, 'encoder_%i.t7'%(epoch+1)))\n",
    "        torch.save(CNet.state_dict(), os.path.join(save_folder, 'CNet_%i.t7'%(epoch+1)))\n",
    "\n",
    "\n",
    "    logger.info(\"Epoch: {}; train \".format(epoch) + log_train_acc_str)\n",
    "    logger.info(\"Epoch: {}; test \".format(epoch) + log_test_acc_str)\n",
    "    \n",
    "    np.save(os.path.join(args.save_path, model_sub_folder, 'source_acc_1_.npy'),source_acc_1_)\n",
    "    np.save(os.path.join(args.save_path, model_sub_folder, 'source_acc_2_.npy'),source_acc_2_)\n",
    "    np.save(os.path.join(args.save_path, model_sub_folder, 'source_acc_3_.npy'),source_acc_3_)\n",
    "    \n",
    "    np.save(os.path.join(args.save_path, model_sub_folder, 'source_test_acc_1_.npy'),source_test_acc_1_)\n",
    "    np.save(os.path.join(args.save_path, model_sub_folder, 'source_test_acc_2_.npy'),source_test_acc_2_)\n",
    "    np.save(os.path.join(args.save_path, model_sub_folder, 'source_test_acc_3_.npy'),source_test_acc_3_)\n",
    "    \n",
    "    np.save(os.path.join(args.save_path, model_sub_folder, 'target_test_acc_.npy'),target_test_acc_)\n",
    "    np.save(os.path.join(args.save_path, model_sub_folder, 'domain_acc_.npy'),domain_acc_)\n",
    "    np.save(os.path.join(args.save_path, model_sub_folder, 'accumulate_domain_loss_.npy'),accumulate_domain_loss_)\n",
    "    \n",
    "    np.save(os.path.join(args.save_path, model_sub_folder, 'loss_source_1_.npy'),loss_source_1_)\n",
    "    np.save(os.path.join(args.save_path, model_sub_folder, 'loss_source_2_.npy'),loss_source_2_)\n",
    "    np.save(os.path.join(args.save_path, model_sub_folder, 'loss_source_3_.npy'),loss_source_3_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
