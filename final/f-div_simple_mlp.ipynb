{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import logging.handlers\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_path DATA_PATH] [--task TASK]\n",
      "                             [--gpu_num GPU_NUM] [--batch_size BATCH_SIZE]\n",
      "                             [--epochs EPOCHS] [--lr_FNN LR_FNN]\n",
      "                             [--lr_encoder LR_ENCODER]\n",
      "                             [--lr_centerloss LR_CENTERLOSS] [--sclass SCLASS]\n",
      "                             [--scent SCENT] [--lr_gan LR_GAN]\n",
      "                             [--target_lbl_percentage TARGET_LBL_PERCENTAGE]\n",
      "                             [--source_lbl_percentage SOURCE_LBL_PERCENTAGE]\n",
      "                             [--num_per_class NUM_PER_CLASS] [--seed SEED]\n",
      "                             [--save_path SAVE_PATH]\n",
      "                             [--model_save_period MODEL_SAVE_PERIOD]\n",
      "                             [--epoch_begin_prototype EPOCH_BEGIN_PROTOTYPE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Tianqin Li\\AppData\\Roaming\\jupyter\\runtime\\kernel-460988ff-287e-4235-b5eb-83de570d2508.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Domain adaptation')\n",
    "parser.add_argument(\"--batch_size\", type=int, default=\"400\", help=\"batch size\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=1e-3, help=\"learning rate\")\n",
    "parser.add_argument(\"--momentum\", type=float, default=0.5, help=\"momentum\")\n",
    "parser.add_argument(\"--gpu_num\", type=int, default=0, help=\"gpu num\")\n",
    "parser.add_argument(\"--seed\", type=int, default=123, help=\"munually set seed\")\n",
    "parser.add_argument(\"--save_path\", type=str, default=\"../train_related\", help=\"save path\")\n",
    "parser.add_argument(\"--subfolder\", type=str, default='domain_origin_dann_svhn_to_mnist', help=\"subfolder name\")\n",
    "parser.add_argument(\"--wtarget\", type=float, default=0.7, help=\"target weight\")\n",
    "parser.add_argument(\"--model_save_period\", type=int, default=2, help=\"save period\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=2000, help=\"label shuffling\")\n",
    "parser.add_argument(\"--dann_weight\", type=float, default=1, help=\"weight for label shuffling\")\n",
    "parser.add_argument(\"--start_origin_dann\", type=int, default=100, help=\"when to start shuffling\")\n",
    "parser.add_argument(\"--use_gpu\", type=int, default=0, help=\"whether using gpu to train\")\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "# snap shot of py file and command\n",
    "python_file_name = sys.argv[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # local only\n",
    "# class local_args:\n",
    "#     def __init__(self, **entries):\n",
    "#         self.__dict__.update(entries)\n",
    "        \n",
    "# args = local_args(**{\n",
    "#     'batch_size': 400,\n",
    "#     'learning_rate': 1e-3,\n",
    "#     'momentum': 0.5,\n",
    "#     'gpu_num': 0,\n",
    "#     'seed': 123,\n",
    "#     'save_path': \"../train_related\",\n",
    "#     'epochs': 2,\n",
    "#     'subfolder': \"testing\",\n",
    "#     'wtarget': 0.7,\n",
    "#     'dann_weight': 1,\n",
    "#     'model_save_period': 2,\n",
    "#     'start_origin_dann': 0,\n",
    "#     'use_gpu': 0,\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.use_gpu == 1 and torch.cuda.is_available():\n",
    "    device = torch.device('cuda:{}'.format(args.gpu_num))\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)\n",
    "# seed\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "cudnn.deterministic = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_sub_folder = args.subfolder + '/origin_dann_weight_%f_learningrate_%f'%(args.dann_weight, args.learning_rate)\n",
    "save_folder = os.path.join(args.save_path, model_sub_folder)\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_path: ../data_unzip\n",
      "task: 3E\n",
      "num_class: 50\n",
      "batch_size: 100\n",
      "num_per_class: 2\n",
      "gap: 5\n",
      "lbl_percentage: 0.7\n",
      "lr_gan: 0.0001\n",
      "lr_FNN: 0.0001\n",
      "lr_encoder: 0.0001\n",
      "epochs: 2\n",
      "clip_value: 0.01\n",
      "n_critic: 4\n",
      "sclass: 0.7\n",
      "scent: 0.01\n",
      "seed: 0\n",
      "save_path: ../train_related\n",
      "model_save_period: 1\n",
      "lr_centerloss: 0.001\n",
      "lr_prototype: 0.001\n",
      "sprototype: 0.01\n",
      "select_pretrain_epoch: 77\n",
      "epoch_begin_prototype: 0\n",
      "sbinary_loss: 1\n",
      "gpu_num: 0\n",
      "source_lbl_percentage: 0.7\n",
      "target_lbl_percentage: 0.7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logfile_path = os.path.join(save_folder, 'logfile.log')\n",
    "if os.path.isfile(logfile_path):\n",
    "    os.remove(logfile_path)\n",
    "    \n",
    "file_log_handler = logging.FileHandler(logfile_path)\n",
    "logger.addHandler(file_log_handler)\n",
    "logger.info(\"Fixed source testing bug\")\n",
    "stdout_log_handler = logging.StreamHandler(sys.stdout)\n",
    "logger.addHandler(stdout_log_handler)\n",
    "\n",
    "attrs = vars(args)\n",
    "for item in attrs.items():\n",
    "    logger.info(\"%s: %s\"%item)\n",
    "logger.info(\"Training Save Path: {}\".format(save_folder))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='../data', train=True, download=True, transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "mnist_testset = datasets.MNIST(root='../data', train=False, download=True, transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "svhn_trainset = datasets.SVHN(root='../data', split='train', download=True, transform=torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.Resize((28, 28)),\n",
    "                    torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize([0.5], [0.5])]))\n",
    "svhn_testset = datasets.SVHN(root='../data', split='test', download=True, transform=torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.Resize((28, 28)),\n",
    "                    torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize([0.5], [0.5])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload data\n",
    "train_mnist_loader = DataLoader(mnist_trainset, batch_size=args.batch_size, shuffle=True)\n",
    "test_mnist_loader = DataLoader(mnist_testset, batch_size=args.batch_size, shuffle=True)\n",
    "train_svhn_loader = DataLoader(svhn_trainset, batch_size=args.batch_size, shuffle=True)\n",
    "test_svhn_loader = DataLoader(svhn_testset, batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "    elif type(m) == nn.LayerNorm:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 30)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, d_in, d_h1, d_h2, d_out, dp=0.2):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_in, d_h1)\n",
    "        self.ln1 = nn.LayerNorm(d_h1)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.dropout1 = nn.Dropout(dp)\n",
    "        self.fc2 = nn.Linear(d_h1, d_h2)\n",
    "        self.ln2 = nn.LayerNorm(d_h2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.dropout2 = nn.Dropout(dp)\n",
    "        self.fc3 = nn.Linear(d_h2, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.ln1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.ln2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def before_lastlinear(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.ln1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.ln2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gfunction(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(Gfunction, self).__init__(\n",
    "            nn.Linear(200,100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(100,1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-093206ff5073>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax_score\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstable_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_mean_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def log_mean_exp(x, device):\n",
    "    max_score = x.max()\n",
    "    batch_size = torch.Tensor([x.shape[0]]).to(device)\n",
    "    stable_x = x - max_score\n",
    "    return max_score - batch_size.log() + stable_x.exp().sum(dim=0).log()\n",
    "\n",
    "a = torch.rand([100,1]).to(device)\n",
    "assert torch.all(log_mean_exp(a, device) - a.exp().mean(dim=0).log() < 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLDiv(g_x_source, g_x_target, device):\n",
    "    # clipping\n",
    "#     g_x_source = torch.clamp(g_x_source, -1e3, 1e3)\n",
    "#     g_x_target = torch.clamp(g_x_target, -1e3, 1e3)\n",
    "    return g_x_source.mean(dim=0) - log_mean_exp(g_x_target, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSDiv(g_x_source, g_x_target, device):\n",
    "    return -F.softplus(-g_x_source).mean(dim=0) - F.softplus(g_x_target).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_class' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-9ad9c236c5b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                                leaky_slope=0.2).to(device)\n\u001b[1;32m     14\u001b[0m \u001b[0mencoder_MLP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFNNSeparated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_h1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_h2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mCNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFNNLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_h2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mGNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mgfunction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_class' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if device == 'cuda:0': \n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "encoder = Encoder().to(device)\n",
    "CNet = FNN(d_in=30, d_h1=100, d_h2=100, d_out=10, dp=0.2).to(device)\n",
    "DomainCNet = FNN(d_in=30, d_h1=100, d_h2=100, d_out=2, dp=0.2).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizerEncoder = optim.Adam(encoder.parameters(), lr=args.learning_rate)\n",
    "optimizerCNet = optim.Adam(CNet.parameters(), lr=args.learning_rate)\n",
    "optimizerDomainCNet = optim.Adam(DomainCNet.parameters(), lr=args.learning_rate)\n",
    "\n",
    "criterion_classifier = nn.CrossEntropyLoss().to(device)\n",
    "# criterion_adverisal = \n",
    "\n",
    "encoder.apply(weights_init)\n",
    "CNet.apply(weights_init)\n",
    "DomainCNet.apply(weights_init)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if args.KL:\n",
    "    gfunction_KL_div_labeled = Gfunction().to(device)\n",
    "    gfunction_KL_div_unlabeled = Gfunction().to(device)\n",
    "\n",
    "if args.JS:\n",
    "    gfunction_JS_div_labeled = Gfunction().to(device)\n",
    "    gfunction_JS_div_unlabeled = Gfunction().to(device)\n",
    "    \n",
    "if args.classifier:\n",
    "    CNet = SimpleMLP2().to(device)\n",
    "    criterion_classifier = nn.CrossEntropyLoss().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in pin memory thread for device 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 31, in _pin_memory_loop\n    data = pin_memory(data)\n  File \"C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 55, in pin_memory\n    return [pin_memory(sample) for sample in data]\n  File \"C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 55, in <listcomp>\n    return [pin_memory(sample) for sample in data]\n  File \"C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 47, in pin_memory\n    return data.pin_memory()\nRuntimeError: cuda runtime error (710) : device-side assert triggered at ..\\aten\\src\\THC\\THCCachingHostAllocator.cpp:278\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-c3fc247cc13f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msource_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabeled_source_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabeled_source_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0moptimizerCNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0moptimizerEncoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\russ-local\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1127\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1128\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1129\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\russ-local\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\russ-local\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    854\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 856\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\russ-local\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 881\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    882\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\russ-local\\lib\\site-packages\\torch\\_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[1;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Caught RuntimeError in pin memory thread for device 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 31, in _pin_memory_loop\n    data = pin_memory(data)\n  File \"C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 55, in pin_memory\n    return [pin_memory(sample) for sample in data]\n  File \"C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 55, in <listcomp>\n    return [pin_memory(sample) for sample in data]\n  File \"C:\\Users\\Tianqin Li\\anaconda3\\envs\\russ-local\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\", line 47, in pin_memory\n    return data.pin_memory()\nRuntimeError: cuda runtime error (710) : device-side assert triggered at ..\\aten\\src\\THC\\THCCachingHostAllocator.cpp:278\n"
     ]
    }
   ],
   "source": [
    "logger.info('Started loading')\n",
    "source_acc_label_ = np.load(os.path.join(args.model_path, 'source_acc_label_.npy'))\n",
    "source_acc_unlabel_ = np.load(os.path.join(args.model_path, 'source_acc_unlabel_.npy'))\n",
    "target_acc_label_ = np.load(os.path.join(args.model_path, 'target_acc_label_.npy'))\n",
    "target_acc_unlabel_ = np.load(os.path.join(args.model_path, 'target_acc_unlabel_.npy'))\n",
    "\n",
    "labeled_KL = []\n",
    "unlabeled_KL = []\n",
    "labeled_JS = []\n",
    "unlabeled_JS = []\n",
    "acc_source_unlabeled_classifier_ = []\n",
    "acc_target_unlabeled_classifier_ = []\n",
    "\n",
    "source_acc_label = []\n",
    "source_acc_unlabel = []\n",
    "target_acc_label = []\n",
    "target_acc_unlabel = []\n",
    "\n",
    "epochs = []\n",
    "\n",
    "for epoch in range(3, source_acc_label_.shape[0], args.intervals*args.model_save_period):\n",
    "    # initialize \n",
    "    if args.KL:\n",
    "        gfunction_KL_div_labeled.apply(weights_init)\n",
    "        optimizer_gfunction_KL_div_labeled = torch.optim.Adam(gfunction_KL_div_labeled.parameters(), lr=args.lr)\n",
    "        gfunction_KL_div_unlabeled.apply(weights_init)\n",
    "        optimizer_gfunction_KL_div_unlabeled = torch.optim.Adam(gfunction_KL_div_unlabeled.parameters(), lr=args.lr)\n",
    "\n",
    "    if args.JS:\n",
    "        gfunction_JS_div_labeled.apply(weights_init)\n",
    "        optimizer_gfunction_JS_div_labeled = torch.optim.Adam(gfunction_JS_div_labeled.parameters(), lr=args.lr)\n",
    "        gfunction_JS_div_unlabeled.apply(weights_init)\n",
    "        optimizer_gfunction_JS_div_unlabeled = torch.optim.Adam(gfunction_JS_div_unlabeled.parameters(), lr=args.lr)\n",
    "\n",
    "    if args.classifier:\n",
    "        CNet.load_state_dict(torch.load(os.path.join(args.model_path, 'CNet_%i.t7'%epoch)))\n",
    "        optimizer_CNet = torch.optim.Adam(CNet.parameters(), lr=args.lr)\n",
    "    \n",
    "    # load weight\n",
    "    encoder.load_state_dict(torch.load(os.path.join(args.model_path, 'encoder_%i.t7'%epoch)))\n",
    "    \n",
    "    # inferencing\n",
    "    encoder.eval()\n",
    "    \n",
    "    # get source/target embedding\n",
    "    source_x_labeled_embedding = torch.empty(0).to(device)\n",
    "    source_y_labeled = torch.empty(0).long().to(device)\n",
    "    source_x_unlabeled_embedding = torch.empty(0).to(device)\n",
    "    source_y_unlabeled = torch.empty(0).long().to(device)\n",
    "    target_x_labeled_embedding = torch.empty(0).to(device)\n",
    "    target_y_labeled = torch.empty(0).long().to(device)\n",
    "    target_x_unlabeled_embedding = torch.empty(0).to(device)\n",
    "    target_y_unlabeled = torch.empty(0).long().to(device)\n",
    "    with torch.no_grad():\n",
    "        for batch_id, (source_x, source_y) in tqdm(enumerate(labeled_source_dataloader), total=len(labeled_source_dataloader)):\n",
    "            source_x = source_x.to(device).view(-1,3200).float()\n",
    "            source_y = source_y.to(device).long()\n",
    "            source_x_embedding = encoder(source_x).detach()\n",
    "            source_x_labeled_embedding = torch.cat([source_x_labeled_embedding, source_x_embedding])\n",
    "            source_y_labeled = torch.cat([source_y_labeled, source_y])\n",
    "            \n",
    "        for batch_id, (source_x, source_y) in tqdm(enumerate(unlabeled_source_dataloader), total=len(unlabeled_source_dataloader)):\n",
    "            source_x = source_x.to(device).view(-1,3200).float()\n",
    "            source_y = source_y.to(device).long()\n",
    "            source_x_embedding = encoder(source_x).detach()\n",
    "            source_x_unlabeled_embedding = torch.cat([source_x_unlabeled_embedding, source_x_embedding])\n",
    "            source_y_unlabeled = torch.cat([source_y_unlabeled, source_y])\n",
    "            \n",
    "        for batch_id, (target_x, target_y) in tqdm(enumerate(labeled_target_dataloader), total=len(labeled_target_dataloader)):\n",
    "            target_x = target_x.to(device).view(-1,3200).float()\n",
    "            target_y = target_y.to(device).long()\n",
    "            fake_x_embedding = encoder(target_x).detach()\n",
    "            target_x_labeled_embedding = torch.cat([target_x_labeled_embedding, fake_x_embedding])     \n",
    "            target_y_labeled = torch.cat([target_y_labeled, target_y])\n",
    "\n",
    "            \n",
    "        for batch_id, (target_x, target_y) in tqdm(enumerate(unlabeled_target_dataloader), total=len(unlabeled_target_dataloader)):\n",
    "            target_x = target_x.to(device).view(-1,3200).float()\n",
    "            target_y = target_y.to(device).long()\n",
    "            fake_x_embedding = encoder(target_x).detach()\n",
    "            target_x_unlabeled_embedding = torch.cat([target_x_unlabeled_embedding, fake_x_embedding])    \n",
    "            target_y_unlabeled = torch.cat([target_y_unlabeled, target_y])\n",
    "            \n",
    "    # for loop to train the gfunction \n",
    "    for i in tqdm(range(args.gfunction_epoch)):\n",
    "        if args.KL:\n",
    "            optimizer_gfunction_KL_div_labeled.zero_grad()\n",
    "            source_x_labeled_g = gfunction_KL_div_labeled(source_x_labeled_embedding)\n",
    "            target_x_labeled_g = gfunction_KL_div_labeled(target_x_labeled_embedding)\n",
    "            loss_KL_labeled = - KLDiv(source_x_labeled_g, target_x_labeled_g, device) # maximize\n",
    "            loss_KL_labeled.backward()\n",
    "            optimizer_gfunction_KL_div_labeled.step()\n",
    "            if i % 500 == 0:\n",
    "                print(\"Epoch %i, Iter %i, labeled KL: %f\"%(epoch, i, -loss_KL_labeled.item()))\n",
    "       \n",
    "        if args.JS:\n",
    "            optimizer_gfunction_JS_div_labeled.zero_grad()\n",
    "            source_x_labeled_g = gfunction_JS_div_labeled(source_x_labeled_embedding)\n",
    "            target_x_labeled_g = gfunction_JS_div_labeled(target_x_labeled_embedding)\n",
    "            loss_JS_labeled = - JSDiv(source_x_labeled_g, target_x_labeled_g, device) # maximize\n",
    "            loss_JS_labeled.backward()\n",
    "            optimizer_gfunction_JS_div_labeled.step()\n",
    "            if i % 500 == 0:\n",
    "                print(\"Epoch %i, Iter %i, labeled JS: %f\"%(epoch, i, -loss_JS_labeled.item()))\n",
    "            \n",
    "    if args.KL:\n",
    "        loss_KL_labeled = - loss_KL_labeled.item()\n",
    "        labeled_KL.append(loss_KL_labeled)\n",
    "      \n",
    "    if args.JS:\n",
    "        loss_JS_labeled = - loss_JS_labeled.item()\n",
    "        labeled_JS.append(loss_JS_labeled)\n",
    "    \n",
    "    for i in tqdm(range(args.gfunction_epoch)):\n",
    "        if args.KL:\n",
    "            optimizer_gfunction_KL_div_unlabeled.zero_grad()\n",
    "            source_x_unlabeled_g = gfunction_KL_div_unlabeled(source_x_unlabeled_embedding)\n",
    "            target_x_unlabeled_g = gfunction_KL_div_unlabeled(target_x_unlabeled_embedding)\n",
    "            loss_KL_unlabeled = - KLDiv(source_x_unlabeled_g, target_x_unlabeled_g, device) # maximize\n",
    "            loss_KL_unlabeled.backward()\n",
    "            optimizer_gfunction_KL_div_unlabeled.step()\n",
    "            if i % 500 == 0:\n",
    "                print(\"Epoch %i, Iter %i, unlabeled KL: %f\"%(epoch, i, -loss_KL_unlabeled.item()))\n",
    "\n",
    "        if args.JS:\n",
    "            optimizer_gfunction_JS_div_unlabeled.zero_grad()\n",
    "            source_x_unlabeled_g = gfunction_JS_div_unlabeled(source_x_unlabeled_embedding)\n",
    "            target_x_unlabeled_g = gfunction_JS_div_unlabeled(target_x_unlabeled_embedding)\n",
    "            loss_JS_unlabeled = - JSDiv(source_x_unlabeled_g, target_x_unlabeled_g, device) # maximize\n",
    "            loss_JS_unlabeled.backward()\n",
    "            optimizer_gfunction_JS_div_unlabeled.step()\n",
    "            if i % 500 == 0:\n",
    "                print(\"Epoch %i, Iter %i, unlabeled JS: %f\"%(epoch, i, -loss_JS_unlabeled.item()))\n",
    "            \n",
    "    if args.KL:  \n",
    "        loss_KL_unlabeled = - loss_KL_unlabeled.item()\n",
    "        unlabeled_KL.append(loss_KL_unlabeled)\n",
    "    \n",
    "    if args.JS:\n",
    "        loss_JS_unlabeled = - loss_JS_unlabeled.item()\n",
    "        unlabeled_JS.append(loss_JS_unlabeled)\n",
    "\n",
    "    acc_source_labeled_classifier = 0\n",
    "    acc_target_labeled_classifier = 0\n",
    "    if args.classifier:\n",
    "#         while i < args.classifier_epoch or (acc_source_labeled_classifier < 0.98 and acc_target_labeled_classifier < 0.98):\n",
    "#             i += 1\n",
    "        for i in tqdm(range(args.classifier_epoch)):\n",
    "            CNet.train()\n",
    "            optimizer_CNet.zero_grad()\n",
    "            pred = CNet(source_x_labeled_embedding)\n",
    "            acc_source_labeled_classifier = (pred.argmax(-1) == source_y_labeled).sum().item() / pred.size(0)\n",
    "            loss_source_classifier_labeled = criterion_classifier(pred, source_y_labeled) * args.sclass\n",
    "            if args.centerloss: loss_source_classifier_labeled += criterion_centerloss(source_x_labeled_embedding, source_y_labeled) * args.scent * args.sclass\n",
    "            loss_source_classifier_labeled.backward()\n",
    "            if args.centerloss: optimizer_centerloss.step()\n",
    "            optimizer_CNet.step()\n",
    "            \n",
    "            optimizer_CNet.zero_grad()\n",
    "            pred = CNet(target_x_labeled_embedding)\n",
    "            acc_target_labeled_classifier = (pred.argmax(-1) == target_y_labeled).sum().item() / pred.size(0)\n",
    "            loss_target_classifier_labeled = criterion_classifier(pred, target_y_labeled)\n",
    "            loss_target_classifier_labeled.backward()\n",
    "            optimizer_CNet.step()\n",
    "            \n",
    "#             if i % 500 == 0:\n",
    "#                 CNet.eval()\n",
    "#                 pred = CNet(source_x_unlabeled_embedding)\n",
    "#                 acc_source_unlabeled_classifier = (pred.argmax(-1) == source_y_unlabeled).sum().item() / pred.size(0)\n",
    "#                 pred = CNet(target_x_unlabeled_embedding)\n",
    "#                 acc_target_unlabeled_classifier = (pred.argmax(-1) == target_y_unlabeled).sum().item() / pred.size(0)\n",
    "#                 print(\"Iter %i: source acc: labeled: %f, unlabeled: %f; target acc: labeled: %f, unlabeled: %f\"%(\n",
    "#                     i, acc_source_labeled_classifier, acc_source_unlabeled_classifier, acc_target_labeled_classifier, acc_target_unlabeled_classifier))\n",
    "        \n",
    "        CNet.eval()\n",
    "        pred = CNet(source_x_unlabeled_embedding)\n",
    "        acc_source_unlabeled_classifier = (pred.argmax(-1) == source_y_unlabeled).sum().item() / pred.size(0)\n",
    "        pred = CNet(target_x_unlabeled_embedding)\n",
    "        acc_target_unlabeled_classifier = (pred.argmax(-1) == target_y_unlabeled).sum().item() / pred.size(0)\n",
    "        acc_source_unlabeled_classifier_.append(acc_source_unlabeled_classifier)\n",
    "        acc_target_unlabeled_classifier_.append(acc_target_unlabeled_classifier)\n",
    "        \n",
    "    # save corresponding acc\n",
    "    source_acc_label.append(source_acc_label_[epoch-1])\n",
    "    source_acc_unlabel.append(source_acc_unlabel_[epoch-1])\n",
    "    target_acc_label.append(target_acc_label_[epoch-1])\n",
    "    target_acc_unlabel.append(target_acc_unlabel_[epoch-1])\n",
    "    epochs.append(epoch)\n",
    "    \n",
    "    logger.info(\"-----------------------------------------\")\n",
    "    log_string = \"Epoch %i: \"%epoch\n",
    "    if args.KL: log_string += \"labeled KL: %f, unlabeled KL: %f; \"%(loss_KL_labeled, loss_KL_unlabeled)\n",
    "    if args.JS: log_string += \"labeled JS: %f, unlabeled JS: %f; \"%(loss_JS_labeled, loss_JS_unlabeled)   \n",
    "    if args.classifier: log_string += \"src unlbl acc: %f, tgt unlbl acc: %f; \"%(acc_source_unlabeled_classifier, acc_target_unlabeled_classifier)      \n",
    "    logger.info(log_string)\n",
    "    logger.info(\"-----------------------------------------\")\n",
    "    \n",
    "    np.save(args.save_path+model_sub_folder+'/epochs.npy', epochs)\n",
    "    np.save(args.save_path+model_sub_folder+'/source_acc_label.npy', source_acc_label)\n",
    "    np.save(args.save_path+model_sub_folder+'/source_acc_unlabel.npy', source_acc_unlabel)\n",
    "    np.save(args.save_path+model_sub_folder+'/target_acc_label.npy', target_acc_label)\n",
    "    np.save(args.save_path+model_sub_folder+'/target_acc_unlabel.npy', target_acc_unlabel)\n",
    "    \n",
    "    if args.KL: \n",
    "        np.save(args.save_path+model_sub_folder+'/labeled_KL.npy', labeled_KL)\n",
    "        np.save(args.save_path+model_sub_folder+'/unlabeled_KL.npy', unlabeled_KL)\n",
    "        \n",
    "    if args.JS:\n",
    "        np.save(args.save_path+model_sub_folder+'/labeled_JS.npy', labeled_JS)\n",
    "        np.save(args.save_path+model_sub_folder+'/unlabeled_JS.npy', unlabeled_JS)\n",
    "        \n",
    "    if args.classifier:\n",
    "        np.save(args.save_path+model_sub_folder+'/acc_source_unlabeled_classifier_.npy', acc_source_unlabeled_classifier_)\n",
    "        np.save(args.save_path+model_sub_folder+'/acc_target_unlabeled_classifier_.npy', acc_target_unlabeled_classifier_)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
